{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ec05d5d",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e1a3db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aidan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from datetime import timedelta\n",
    "import os\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "from collections import Counter, defaultdict\n",
    "from string import punctuation\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "sw = stopwords.words('english')\n",
    "import janitor\n",
    "import spacy\n",
    "from textblob import TextBlob\n",
    "tb = TextBlob('')\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import kruskal\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import Layout\n",
    "from ipywidgets import interact_manual, interactive_output\n",
    "from IPython.display import display, clear_output\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n",
    "import spacy\n",
    "import math\n",
    "from textblob import TextBlob\n",
    "tb = TextBlob('')\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from random import randint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import winsound\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from spellchecker import SpellChecker\n",
    "from textblob import TextBlob\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "import warnings\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import words\n",
    "from nltk.util import ngrams\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b112e649",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_ngrams(text, n):\n",
    "    ### Function that takes in # of n for ngrams, and text to be processed. Then counts and sorts ngrams to \n",
    "    ### find the most common ngrams!\n",
    "    tokens = word_tokenize(text)  \n",
    "    n_grams = ngrams(tokens, n)\n",
    "    n_gram_freq = Counter(n_grams)\n",
    "    sorted_n_grams = dict(sorted(n_gram_freq.items(), key=lambda item: item[1], reverse=True))\n",
    "    \n",
    "    \n",
    "    return sorted_n_grams\n",
    "\n",
    "def completion_percentage(df, columns_to_process):\n",
    "    ### A function that finds various completion percentages for the text data to inform the reader.\n",
    "    completion_dict = {}\n",
    "\n",
    "    for column in columns_to_process:\n",
    "        total_count = len(df)\n",
    "        non_empty_count = df[column].apply(lambda x: x.strip() if isinstance(x, str) else x).replace('', pd.NA).notna().sum()\n",
    "        \n",
    "        if total_count > 0:\n",
    "            perc_completion = non_empty_count / total_count * 100\n",
    "        else:\n",
    "            perc_completion = 0\n",
    "        \n",
    "        completion_dict[column] = perc_completion\n",
    "    print(\"Below is the percantage of entires with at least some unique text for a fire-risk grouping:\")\n",
    "    return completion_dict\n",
    "\n",
    "def basic_text(df, columns_to_process):\n",
    "    ### A function to compile very basic text analyses\n",
    "    results = {}\n",
    "    \n",
    "    for column in columns_to_process:\n",
    "        concatenated_text = df[column].astype(str).str.cat(sep=' ')\n",
    "\n",
    "        text_clean = [word for word in concatenated_text.split()]\n",
    "        c = Counter(text_clean)\n",
    "\n",
    "        total_tokens = len(text_clean)\n",
    "        unique_tokens = len(set(text_clean))\n",
    "\n",
    "        lex_diversity = unique_tokens / total_tokens if total_tokens != 0 else \"NA\"\n",
    "        avg_token_len = np.mean([len(word) for word in text_clean])\n",
    "        top_10 = c.most_common(10)\n",
    "\n",
    "        avg_tokens_per_entry = df[column].apply(lambda x: len(str(x).split())).mean()\n",
    "        \n",
    "        \n",
    "        results[column] = {\n",
    "            'total_tokens': total_tokens,\n",
    "            'unique_tokens': unique_tokens,\n",
    "            'average_tokens' : avg_tokens_per_entry,\n",
    "            'avg_token_length': avg_token_len,\n",
    "            'lexical_diversity': lex_diversity,\n",
    "            'top_10': top_10,\n",
    "        }\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "367482ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_target_word(text, target_word):\n",
    "    return target_word in text\n",
    "\n",
    "def percentage_with_target_words(data, target_words):\n",
    "    percentage_with_target_words = {}\n",
    "\n",
    "    data_copy = data.copy()\n",
    "\n",
    "    for word in target_words:\n",
    "        data_copy[f'contains_{word}'] = data_copy['combined_text'].apply(contains_target_word, target_word=word)\n",
    "        percentage_with_target_words[word] = (data_copy[f'contains_{word}'].sum() / len(data_copy)) * 100\n",
    "        data_copy.drop(columns=[f'contains_{word}'], inplace=True)\n",
    "\n",
    "    sorted_results = sorted(percentage_with_target_words.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    for word, percentage in sorted_results:\n",
    "        print(f\"The word '{word}' appears in {percentage:.2f}% of the combined notes fields for fires.\")\n",
    "        \n",
    "def percentage_by_risk(data, target_words):\n",
    "    results_by_risk = {}\n",
    "\n",
    "    data_copy = data.copy()\n",
    "\n",
    "    custom_order = ['high', 'mod', 'low']\n",
    "\n",
    "    grouped_data = data_copy.groupby('rrf_rr_desc', sort=False)  # Use sort=False to maintain custom order\n",
    "\n",
    "    for risk, group in grouped_data:\n",
    "        percentage_with_target_words = {}\n",
    "        for word in target_words:\n",
    "            group[f'contains_{word}'] = group['combined_text'].apply(contains_target_word, target_word=word)\n",
    "            percentage_with_target_words[word] = (group[f'contains_{word}'].sum() / len(group)) * 100\n",
    "            group.drop(columns=[f'contains_{word}'], inplace=True)\n",
    "        results_by_risk[risk] = percentage_with_target_words\n",
    "\n",
    "    for risk in custom_order:\n",
    "        if risk in results_by_risk:\n",
    "            percentages = results_by_risk[risk]\n",
    "            print(f\"Risk: {risk.capitalize()}\")\n",
    "            for word, percentage in sorted(percentages.items(), key=lambda x: x[1], reverse=True):\n",
    "                print(f\"The word '{word}' appears in {percentage:.2f}% of the texts for this risk.\")\n",
    "            print()\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "def extract_context(text, target_word, context_window_size = 5):\n",
    "    words = text.split()\n",
    "    target_indices = [i for i, word in enumerate(words) if word.lower() == target_word]\n",
    "    \n",
    "    contexts = []\n",
    "    for index in target_indices:\n",
    "        start_index = max(0, index - context_window_size)\n",
    "        end_index = min(len(words), index + context_window_size + 1)\n",
    "        context = words[start_index:end_index]\n",
    "        contexts.append(context)\n",
    "    \n",
    "    return contexts\n",
    "\n",
    "def extract_ordered_ngrams(context_list, n =3):\n",
    "    ordered_trigrams = []\n",
    "    for context in context_list:\n",
    "        ordered_trigrams.extend(\" \".join(context[i:i+n]) for i in range(len(context) - 2))\n",
    "    return ordered_trigrams\n",
    "\n",
    "def vectorize_text_with_ngrams(text_data, labels, ngram_range=(1, 2), test_size=0.2):\n",
    "    \"\"\"\n",
    "    Vectorize text data using TF-IDF with specified n-gram range.\n",
    "\n",
    "    Parameters:\n",
    "    - text_data: List of text data.\n",
    "    - labels: List of corresponding labels.\n",
    "    - ngram_range: Tuple specifying the range of n-grams (default is (1, 2)).\n",
    "    - test_size: Fraction of the dataset to be used as the test set (default is 0.2).\n",
    "    - random_state: Seed for random number generation (default is None).\n",
    "\n",
    "    Returns:\n",
    "    - Tuple (train_tfidf, test_tfidf, y_train, y_test)\n",
    "      - train_tfidf: TF-IDF transformed training data.\n",
    "      - test_tfidf: TF-IDF transformed test data.\n",
    "      - y_train: Training labels.\n",
    "      - y_test: Test labels.\n",
    "    \"\"\"\n",
    "    # Split the data\n",
    "    x_train, x_test, y_train, y_test = train_test_split(text_data, labels, test_size=test_size, random_state= 2)\n",
    "\n",
    "    # Vectorize using TF-IDF\n",
    "    vectorizer = TfidfVectorizer(ngram_range=ngram_range)\n",
    "    x_train_tfidf = vectorizer.fit_transform(x_train)\n",
    "    x_test_tfidf = vectorizer.transform(x_test)\n",
    "\n",
    "    return x_train_tfidf, x_test_tfidf, y_train, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caaeb92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_tfidf_vectors(tfidf_matrix, indices):\n",
    "    \"\"\"\n",
    "    Computes the average TF-IDF vector for the given indices in the TF-IDF matrix.\n",
    "\n",
    "    Parameters:\n",
    "    tfidf_matrix (numpy.ndarray): The TF-IDF matrix.\n",
    "    indices (list): List of indices to compute the average for.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The average TF-IDF vector.\n",
    "    \"\"\"\n",
    "    selected_vectors = tfidf_matrix[indices]\n",
    "    average_vector = np.mean(selected_vectors, axis=0)\n",
    "    return average_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cde3dbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1438d6f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9df474",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
